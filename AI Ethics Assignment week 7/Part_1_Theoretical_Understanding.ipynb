{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ⚖️ Part 1: Theoretical Understanding\n",
        "\n",
        "## 1. Short Answer Questions\n",
        "\n",
        "### Q1: Define algorithmic bias and provide two examples of how it manifests in AI systems.\n",
        "\n",
        "**Definition of Algorithmic Bias:**\n",
        "Algorithmic bias refers to systematic and repeatable errors in an AI system that create unfair outcomes, such as favoring one arbitrary group of users or data points over others. This bias is not a conscious intent of the algorithm itself, but rather a reflection of societal, systemic, or operational prejudices that are embedded in the data used to train the model, the assumptions made during the design process, or the way the model is deployed and interacted with.\n",
        "\n",
        "**Two Examples of Manifestation:**\n",
        "\n",
        "-   **Skewed Representation (Historical Bias in Data):**\n",
        "    -   **Example:** Facial recognition systems trained predominantly on images of lighter-skinned individuals and male faces often exhibit significantly lower accuracy rates for women and people with darker skin tones. This is a direct consequence of the historical dataset reflecting a demographic imbalance (or poor data collection diversity), causing the model to learn and reproduce the training data's limitations. For example, studies have shown error rates up to **34% higher for darker-skinned women** compared to lighter-skinned men.\n",
        "-   **Harmful Feedback Loops (Selection/Interaction Bias):**\n",
        "    -   **Example:** Predictive policing algorithms, which estimate crime likelihood in specific geographical areas. If police historically focus on and arrest people in low-income or minority neighborhoods (a systemic bias), the resulting crime data will show a higher density of arrests in those areas. The AI model then learns this pattern and predicts that these areas are high-crime, justifying increased police presence, which leads to more arrests, thus reinforcing the initial, biased data in a vicious **feedback loop**. The system perpetuates, rather than corrects, existing inequities.\n",
        "\n",
        "### Q2: Explain the difference between transparency and explainability in AI. Why are both important?\n",
        "\n",
        "| Feature | Transparency (Openness) | Explainability (Intelligibility) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Focus** | How the system works structurally (e.g., data, code, processes). | Why a specific decision was made (e.g., feature importance). |\n",
        "| **Question** | *What* is the model doing? | *Why* did the model output X for input Y? |\n",
        "| **Stakeholder** | Regulators, developers, auditors. | End-users, affected individuals, internal teams. |\n",
        "| **Examples** | Open-sourcing the training dataset, documenting model architecture (e.g., \"We used a Random Forest model on features A, B, and C\"). | Generating a feature importance chart showing which inputs drove a lending decision, or providing a counterfactual explanation (e.g., \"If you had earned \\$10,000 more, your loan would have been approved\"). |\n",
        "\n",
        "**Importance of Both:**\n",
        "\n",
        "-   **Transparency** is crucial for accountability and auditing. It allows external parties to scrutinize the design process, ensuring no unethical shortcuts were taken and that the system complies with regulations.\n",
        "-   **Explainability (XAI)** is essential for building trust and enabling recourse. If a user is denied a job or a loan, knowing *why* allows them to challenge the decision or change their behavior (the **\"right to explanation\"** concept). Both together empower humans to govern AI systems effectively.\n",
        "\n",
        "### Q3: How does GDPR impact AI development in the EU?\n",
        "\n",
        "The General Data Protection Regulation (GDPR) significantly impacts AI development within the EU (and for companies handling EU data) primarily through two key articles:\n",
        "\n",
        "-   **Lawfulness, Fairness, and Transparency (Article 5):** This core principle mandates that all processing (including AI training and inference) must be lawful and transparent. Developers must clearly inform users about how their data is used, which directly affects the transparency of data collection and model training practices.\n",
        "-   **The Right to Explanation/Restriction on Automated Decision Making (Article 22):** This is the most direct impact. It states that individuals have the right not to be subject solely to a decision based on automated processing (like an AI model) if that decision produces legal effects or similarly significant effects concerning them. When such automated decisions *are* made, the user must be informed and often has the right to obtain meaningful human intervention and an explanation of the decision's rationale. This pushes developers toward using **explainable AI (XAI)** techniques.\n",
        "\n",
        "## 2. Ethical Principles Matching\n",
        "\n",
        "**Match the following principles to their definitions:**\n",
        "\n",
        "-   Ensuring AI does not harm individuals or society. **-> B) Non-maleficence**\n",
        "-   Respecting users’ right to control their data and decisions. **-> C) Autonomy**\n",
        "-   Designing AI to be environmentally friendly. **-> D) Sustainability**\n",
        "-   Fair distribution of AI benefits and risks. **-> A) Justice**\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "4aavu2I54zqJ"
      }
    }
  ]
}
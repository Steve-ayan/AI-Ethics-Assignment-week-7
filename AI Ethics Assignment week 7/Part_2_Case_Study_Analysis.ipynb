{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåç Part 2: Case Study Analysis\n",
        "\n",
        "## Case 1: Biased Hiring Tool\n",
        "\n",
        "**Scenario:** Amazon‚Äôs AI recruiting tool penalized female candidates, leading to unfair scoring for technical roles.\n",
        "\n",
        "### Task 1: Identify the source of bias\n",
        "\n",
        "The primary source of bias was **Historical Data Bias (Skewed Representation)**, manifesting in the following ways:\n",
        "\n",
        "-   **Training Data Skew:** The system was trained on over a decade of r√©sum√©s submitted to Amazon. Since the tech industry, and historical successful hires at Amazon, were heavily **male-dominated**, the model learned that male-associated characteristics were indicators of success.\n",
        "-   **Proxy Discrimination:** The model inadvertently picked up on gender-correlated features (or **proxy variables**) such as attending an all-women's college or mentioning the word \"woman's\" in club descriptions, and assigned them negative weights. The model was fair on the surface but biased at its core.\n",
        "-   **Tainted Labeling:** The historical hiring outcome (the \"label\") itself was a product of past systemic human bias. The AI merely learned to replicate and reinforce those biased hiring decisions.\n",
        "\n",
        "### Task 2: Propose three fixes to make the tool fairer\n",
        "\n",
        "-   **1. Data Augmentation and Counterfactual Fairness (Pre-processing):**\n",
        "    -   Systematically **rebalance the training dataset**. This involves finding pairs of candidates who were objectively equally qualified but differed in protected attributes (like gender) and ensuring the model sees them with equal outcome labels. Techniques like **Data Augmentation** (creating synthetic records) or **Reweighting** can be used to achieve demographic parity in the training set.\n",
        "-   **2. Feature Neutralization and Removal (In-processing):**\n",
        "    -   Implement strict filtering to **remove all direct and proxy variables** related to gender (e.g., specific college names, gendered pronouns, first names). Use in-processing debiasing techniques (like **Adversarial Debiasing**) that actively train the model to be blind to the protected attribute while making predictions.\n",
        "-   **3. Human-in-the-Loop & Auditing (Post-processing/Deployment):**\n",
        "    -   Use the AI tool strictly as a **screening and prioritization aid**, not a decision-maker. Mandate that final hiring decisions and shortlisting must involve **human recruiters** trained in unconscious bias mitigation. A regular, independent **Fairness Audit** must be conducted quarterly to measure demographic outcomes.\n",
        "\n",
        "### Task 3: Suggest metrics to evaluate fairness post-correction\n",
        "\n",
        "These metrics quantify the disparity in outcomes between the privileged group (historically men) and the unprivileged group (historically women).\n",
        "\n",
        "-   **Disparate Impact Ratio (DIR):**\n",
        "    $$DIR = \\frac{\\text{Selection Rate}_{\\text{Unprivileged}}}{\\text{Selection Rate}_{\\text{Privileged}}}$$\n",
        "    -   *Goal:* The DIR should be close to 1 (ideally between 0.8 and 1.25). A value significantly below 0.8 is considered strong evidence of bias (adverse impact).\n",
        "-   **Equal Opportunity Difference (EOD):**\n",
        "    $$EOD = P(\\hat{Y}=1 \\mid Y=1, A=\\text{unprivileged}) - P(\\hat{Y}=1 \\mid Y=1, A=\\text{privileged})$$\n",
        "    -   *Goal:* The EOD should be close to 0. This metric focuses on the **False Negative Rate**‚Äîit ensures that candidates who *are truly qualified* ($Y=1$) have an equal chance of being selected ($\\hat{Y}=1$), regardless of their gender ($A$).\n",
        "\n",
        "---\n",
        "\n",
        "## Case 2: Facial Recognition in Policing\n",
        "\n",
        "**Scenario:** A facial recognition system misidentifies minorities at higher rates.\n",
        "\n",
        "### Task 1: Discuss ethical risks\n",
        "\n",
        "The use of biased Facial Recognition (FR) in policing poses severe ethical and societal risks, fundamentally violating core AI principles:\n",
        "\n",
        "-   **Violation of Justice (Inaccurate Outcomes):** Higher **False Positive Rates (FPR)** for minority groups directly lead to an increased probability of misidentification, resulting in unwarranted stops, wrongful arrests, and unjust incarceration.\n",
        "-   **Erosion of Autonomy and Privacy:** FR systems enable mass, persistent surveillance of individuals in public spaces. This creates a **chilling effect** on the exercise of free speech and assembly, as citizens feel they are constantly monitored, violating the principle of **Autonomy**.\n",
        "-   **Non-maleficence and Systemic Harm:** Deployment often targets communities already facing over-policing. The technology validates and intensifies this bias, disproportionately causing psychological and material harm (e.g., job loss, trauma) to targeted communities.\n",
        "-   **Lack of Recourse and Transparency:** Since these models are often proprietary (**\"black boxes\"**), it becomes nearly impossible for an individual to challenge or understand *why* they were misidentified, hindering the right to due process.\n",
        "\n",
        "### Task 2: Recommend policies for responsible deployment\n",
        "\n",
        "-   **1. Mandate Auditable Performance Standards:**\n",
        "    -   **Policy:** Deployment must be contingent on an **independent, third-party audit** showing near-parity in error rates (**FPR** and **FNR**) across all demographic subgroups. FR systems that exhibit disparate accuracy should be rejected or deactivated until corrected.\n",
        "-   **2. FR as Investigative Lead Only (Human-in-the-Loop):**\n",
        "    -   **Policy:** Facial recognition matches **must never** constitute sufficient probable cause for arrest or detention. The match can only be used as an investigative lead, requiring independent, human-verified evidence (e.g., human corroboration by a minimum of two trained analysts, physical evidence) before any action is taken.\n",
        "-   **3. Moratorium on Continuous, Real-Time Surveillance:**\n",
        "    -   **Policy:** Ban the use of real-time, continuous facial recognition surveillance in public spaces (e.g., constant scanning of crowds). Limit usage strictly to **post-incident forensic analysis** where an individual suspect has already been identified and there is a direct threat to public safety.\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "UkvP9nlqA5Ev"
      }
    }
  ]
}